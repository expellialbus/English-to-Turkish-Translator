import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Layer, MultiHeadAttention, LayerNormalization, Dense


class TransformerDecoder(Layer):
    """
    Decoder part of and encoder-decoder architecture with attention mechanism

    Attributes
    ----------
    num_heads : int
                Number of heads for MultiHeadAttention

    embed_dim : int
                Dimension size used within embedding layer

    dense_units : int
                  Number of units will be used inside dense layer

    **kwargs : dict
               Remain key-value pairs for a keras.layers.Layer object

    Methods
    -------
    get_causal_attention_mask(self, inputs)
        Creates mask for target sentences to avoid decoder
        looking to the "N + 1" token during training

    call(self, inputs, encoder_outputs, mask=None)
        Function used for computation, see keras docs for more information

    get_config(self)
        See https://keras.io/api/models/model_saving_apis/#getconfig-method


    Notes
    -----
    Also see https://keras.io/guides/making_new_layers_and_models_via_subclassing/

    """

    def __init__(self, num_heads, embed_dim, dense_units, **kwargs):
        super().__init__(**kwargs)

        self.num_heads = num_heads
        self.embed_dim = embed_dim
        self.dense_units = dense_units

        self.first_attention = MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.second_attention = MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )

        # also called as dense projections
        self.dense = Sequential(
            [
                Dense(dense_units, activation="relu"),
                # to be able concatenate outputs with inputs
                # output layer of dense projections should have same size with input dimension
                # embed_dim in this case
                Dense(embed_dim),
            ]
        )

        self.first_layer_norm = LayerNormalization()
        self.second_layer_norm = LayerNormalization()
        self.third_layer_norm = LayerNormalization()

        self.supports_masking = True

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]

        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)

        # creates mask for target sentences to avoid decoder looking to the "N + 1" token
        # during training, operation ends up like below
        # 1, 0, 0, ... to N
        # 0, 1, 0, ... to N
        # 0, 0, 1, ... to N
        #       .
        #       .
        #       .
        #      to
        #       N
        mask = tf.cast(i >= j, dtype=tf.int32)[tf.newaxis, :]
        tile = tf.constant([batch_size, 1, 1])

        return tf.tile(mask, tile)

    def call(self, inputs, encoder_outputs, mask=None):
        causal_mask = get_causal_attention_mask(inputs)

        if mask is not None:
            # The mask generated by Embedding layer will be 2D
            # but Attention layer expects mask to be 3D or 4D
            # so this operation adds a new axis around samples in the batch

            # since mask is composed of booleans, it has to be converted to int
            # to be able to combine with causal mask
            mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)

            # creates the final mask
            # which is a combination of both padding mask (mask)
            # and time step mask (causal_mask)
            padding_mask = tf.minimum(mask, causal_mask)

        first_attention_output = self.first_attention(
            query=inputs, key=inputs, value=inputs, attention_mask=causal_mask
        )
        first_atttention_output = self.first_layer_norm(inputs + first_attention_output)

        second_attention_output = self.second_attention(
            query=first_attention_output,
            key=encoder_outputs,
            value=encoder_outputs,
            attention_mask=causal_mask,
        )
        second_attention_output = self.second_layer_norm(
            first_attention_output + second_attention_output
        )

        proj_output = self.dense(second_atttention_output)
        output = self.third_layer_norm(second_attention_output + proj_output)

        return output

    def get_config(self):
        config = super().get_config()

        config.update(
            {
                "num_heads": self.num_heads,
                "embed_dim": self.embed_dim,
                "dense_units": self.dense_units,
            }
        )

        return config
